Base Model Used: EleutherAI GPT-1.3B

Dataset Used for Fine Tuning: Wikitext Dataset (a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia)

Methods Used: LoRA (Low Rank Approximation) and 4 - bit Quantisation
